<a href="https://github.com/drshahizan/short-course/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/short-course" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/short-course/network/members"><img src="https://img.shields.io/github/forks/drshahizan/short-course" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/short-course/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/short-course" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/short-course"><img src="https://img.shields.io/github/issues/drshahizan/short-course" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/short-course/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/short-course?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2Fshort-course&labelColor=%23d9e3f0&countColor=%23697689&style=flat)

# Session 8: Ethical and Responsible Use of AI

This final session is crucial for providing a balanced perspective on AI. It moves beyond the practical applications to address the critical ethical considerations and societal impacts of this technology. The goal is to equip users with the knowledge and principles necessary to use AI responsibly, recognize its limitations, and mitigate potential harm.

## Avoiding Misinformation

Large Language Models (LLMs) and other generative AI tools are powerful, but they are not infallible. They can sometimes generate inaccurate or fabricated information, a phenomenon often referred to as "hallucination." This section teaches users how to be critical consumers and producers of AI-generated content.

* **Understanding AI's Limitations:** The session will explain that AI models do not "know" facts in the same way humans do. They generate text by predicting the most likely sequence of words based on their training data. This process can sometimes produce plausible-sounding but completely false information.
* **The Importance of Fact-Checking:** A key takeaway is that all AI-generated information, especially factual claims, must be verified using reliable, external sources. This reinforces the need for human oversight.
* **Practical Examples:** Learners can be shown examples of AI-generated misinformation. For instance, a prompt could ask an AI to "provide three facts about the first person to walk on the moon." The AI might generate a plausible-sounding but incorrect fact, which the user can then identify and correct by cross-referencing with a reputable source like a NASA website.
* **Responsible Prompting:** The session will also cover how to write prompts that encourage more accurate and cautious responses, such as asking the AI to "provide a source for each claim" or to "state when it is unsure of an answer."

## Addressing Bias

AI models are trained on vast datasets that reflect human history and society, which are unfortunately filled with biases (e.g., gender, racial, cultural). Without proper safeguards, these biases can be perpetuated or even amplified by AI.

* **Identifying Bias in AI Output:** The session will provide examples of biased AI outputs. For instance, a user could prompt an AI to "write a story about a doctor and a nurse" and observe if the AI defaults to gender stereotypes (e.g., the doctor is male, the nurse is female). Another example could involve asking for images of "CEOs" and seeing if the AI defaults to a certain race or gender.
* **Understanding the Source of Bias:** The session will explain that the bias isn't "malicious" in the AI itself; it's a reflection of the data it was trained on. This understanding is crucial for addressing the root cause.
* **Mitigating Bias Through Prompting:** Learners will be taught how to write prompts that explicitly counteract bias. For example, instead of a generic prompt, a user could write: "Write a story about a nurse and a doctor. Ensure that the doctor is female and the nurse is male."
* **Ethical AI Development and Auditing:** The session will briefly touch on the ongoing efforts within the AI community to audit models for bias and develop new techniques to create fairer and more inclusive AI.

## Privacy and Confidentiality

When using AI tools, especially those that process user inputs, it is critical to be aware of the data privacy implications. The information users provide can be used to train future models, and there are risks associated with sharing sensitive data.

* **The "Golden Rule" of AI Use:** A core principle is to **never input sensitive, personal, or confidential information** into a public AI model. This includes personal addresses, financial data, internal business reports, and proprietary intellectual property.
* **Understanding Data Usage Policies:** The session will encourage users to read the data usage and privacy policies of the AI tools they use. This is crucial for understanding how their data is being collected, stored, and used.
* **Confidentiality in the Workplace:** For professional use, the session will emphasize the importance of adhering to company policies regarding the use of AI. Many organizations have strict rules about what can and cannot be shared with external AI services.
* **The Risk of Data Leaks:** The session will explain that even seemingly innocuous information, when combined with other data, can pose a risk. The goal is to foster a habit of caution and discretion when interacting with AI platforms.

By the end of this session, participants will have a strong ethical foundation for using AI. They will not only be proficient in using the tools but also be responsible digital citizens who can navigate the complexities of AI with awareness and integrity. This session completes the curriculum by shifting the focus from "how to use AI" to "how to use AI ethically and responsibly."

### üôåüèª Connect with Me
<p align="left">
    <a href="https://github.com/drshahizan" target="_blank"><img alt="GitHub" src="https://img.shields.io/badge/-@drshahizan-181717?style=flat-square&logo=GitHub&logoColor=white"></a>
    <a href="https://www.linkedin.com/in/drshahizan" target="_blank"><img alt="LinkedIn" src="https://img.shields.io/badge/-drshahizan-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/drshahizan/"></a>
    <a href="mailto:shahizan@utm.my" target="_blank"><img alt="Email" src="https://img.shields.io/badge/-shahizan@utm.my-c14438?style=flat-square&logo=Gmail&logoColor=white&link=mailto:shahizan@utm.my.com"></a>
    <a href="https://www.researchgate.net/profile/Mohd-Othman-28" target="_blank"><img alt="ResearchGate" src="https://img.shields.io/badge/-ResearchGate-00CCBB?style=flat-square&logo=ResearchGate&logoColor=white"></a>
    <a href="https://orcid.org/0000-0003-4261-1873" target="_blank"><img alt="ORCID" src="https://img.shields.io/badge/-ORCID-A6CE39?style=flat-square&logo=ORCID&logoColor=white"></a> 
 <a href="https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan" target="_blank"><img alt="A" src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic"></a>
 
![](https://hit.yhype.me/github/profile?user_id=81284918)
</p>

